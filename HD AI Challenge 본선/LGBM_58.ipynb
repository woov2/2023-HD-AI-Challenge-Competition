{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27219668",
   "metadata": {},
   "source": [
    "# Environment\n",
    "- GPU : NVIDIA GeForce GTX 1060\n",
    "- CPU : Intel CORE i5 8th Gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db038ba1",
   "metadata": {},
   "source": [
    "## Library version check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903ae4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tqdm as tq\n",
    "import lightgbm as lgb\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(\"-------------------------- Python & library version --------------------------\")\n",
    "print(\"Python version: {}\".format(sys.version))\n",
    "print(\"pandas version: {}\".format(pd.__version__))\n",
    "print(\"numpy version: {}\".format(np.__version__))\n",
    "print(\"matplotlib version: {}\".format(matplotlib.__version__))\n",
    "print(\"tqdm version: {}\".format(tq.__version__))\n",
    "print(\"lightgbm version: {}\".format(lgb.__version__))\n",
    "print(\"seaborn version: {}\".format(sns.__version__))\n",
    "print(\"scikit-learn version: {}\".format(skl.__version__))\n",
    "print(\"------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7ba370",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fbfa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "\n",
    "pd.set_option('display.max_columns', 30)\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf6a469",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b7cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = glob.glob('./train/*.csv')\n",
    "test_paths = pd.read_csv('./test.csv')['data_path'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e0fa65",
   "metadata": {},
   "source": [
    "## Data Reconstructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5423b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame()\n",
    "for path in tqdm(train_paths):\n",
    "        driver = str(path.split('/')[-1].split('.')[0].split('_')[1][0])\n",
    "        data = pd.read_csv(path)\n",
    "        \n",
    "        data['diff_1'] = (data['Signal A'] - data['Signal A'].shift(1)).fillna(0)\n",
    "        data['diff_2'] = (data['Signal B'] - data['Signal B'].shift(1)).fillna(0)\n",
    "        data['diff_3'] = (data['Signal C'] - data['Signal C'].shift(1)).fillna(0)\n",
    "        data['diff_4'] = (data['Sensor A'] - data['Sensor A'].shift(1)).fillna(0)\n",
    "        data['diff_5'] = (data['Sensor B'] - data['Sensor B'].shift(1)).fillna(0)\n",
    "        data['diff_6'] = (data['Sensor C'] - data['Sensor C'].shift(1)).fillna(0)\n",
    "        data['diff_7'] = (data['Sensor D'] - data['Sensor D'].shift(1)).fillna(0)\n",
    "\n",
    "        data['rolling_diff_1'] = data['diff_1'].rolling(5).sum().bfill()\n",
    "        data['rolling_diff_2'] = data['diff_2'].rolling(5).sum().bfill()\n",
    "        data['rolling_diff_3'] = data['diff_3'].rolling(5).sum().bfill()\n",
    "        data['rolling_diff_4'] = data['diff_4'].rolling(5).sum().bfill()\n",
    "        data['rolling_diff_5'] = data['diff_5'].rolling(5).sum().bfill()\n",
    "        data['rolling_diff_6'] = data['diff_6'].rolling(5).sum().bfill()\n",
    "        data['rolling_diff_7'] = data['diff_7'].rolling(5).sum().bfill()\n",
    "        \n",
    "        data['rolling_diff_1_std'] = data['diff_1'].rolling(5).std().bfill()\n",
    "        data['rolling_diff_2_std'] = data['diff_2'].rolling(5).std().bfill()\n",
    "        data['rolling_diff_3_std'] = data['diff_3'].rolling(5).std().bfill()\n",
    "        data['rolling_diff_4_std'] = data['diff_4'].rolling(5).std().bfill()\n",
    "        data['rolling_diff_5_std'] = data['diff_5'].rolling(5).std().bfill()\n",
    "        data['rolling_diff_6_std'] = data['diff_6'].rolling(5).std().bfill()\n",
    "        data['rolling_diff_7_std'] = data['diff_7'].rolling(5).std().bfill()\n",
    "        \n",
    "        data['driver'] = 0 if driver == 'A' else 1\n",
    "        label = float(path.split('\\\\')[-1].split('.')[0].split('_')[0][:-2])\n",
    "        data['label'] = label\n",
    "        train = pd.concat([train, data], axis = 0)\n",
    "        \n",
    "train.columns = ['time','signal_A','signal_B','signal_C','sensor_A','sensor_B','sensor_C','sensor_D',\n",
    "                 'diff_1','diff_2','diff_3','diff_4','diff_5','diff_6','diff_7',\n",
    "                 'rolling_diff_1','rolling_diff_2','rolling_diff_3','rolling_diff_4','rolling_diff_5','rolling_diff_6','rolling_diff_7',\n",
    "                 'rolling_diff_1_std','rolling_diff_2_std','rolling_diff_3_std','rolling_diff_4_std','rolling_diff_5_std','rolling_diff_6_std','rolling_diff_7_std',\n",
    "                 'driver','label']\n",
    "train = train.reset_index(drop=True)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f63618",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for path in tqdm(test_paths):\n",
    "        driver = str(path.split('/')[-1].split('.')[0].split('_')[1][0])\n",
    "        data = pd.read_csv(path)\n",
    "        \n",
    "        data['diff_1'] = (data['Signal A'] - data['Signal A'].shift(1)).fillna(0)\n",
    "        data['diff_2'] = (data['Signal B'] - data['Signal B'].shift(1)).fillna(0)\n",
    "        data['diff_3'] = (data['Signal C'] - data['Signal C'].shift(1)).fillna(0)\n",
    "        data['diff_4'] = (data['Sensor A'] - data['Sensor A'].shift(1)).fillna(0)\n",
    "        data['diff_5'] = (data['Sensor B'] - data['Sensor B'].shift(1)).fillna(0)\n",
    "        data['diff_6'] = (data['Sensor C'] - data['Sensor C'].shift(1)).fillna(0)\n",
    "        data['diff_7'] = (data['Sensor D'] - data['Sensor D'].shift(1)).fillna(0)\n",
    "\n",
    "        data['rolling_diff_1'] = data['diff_1'].rolling(5).sum().bfill()\n",
    "        data['rolling_diff_2'] = data['diff_2'].rolling(5).sum().bfill()\n",
    "        data['rolling_diff_3'] = data['diff_3'].rolling(5).sum().bfill()\n",
    "        data['rolling_diff_4'] = data['diff_4'].rolling(5).sum().bfill()\n",
    "        data['rolling_diff_5'] = data['diff_5'].rolling(5).sum().bfill()\n",
    "        data['rolling_diff_6'] = data['diff_6'].rolling(5).sum().bfill()\n",
    "        data['rolling_diff_7'] = data['diff_7'].rolling(5).sum().bfill()\n",
    "        \n",
    "        data['rolling_diff_1_std'] = data['diff_1'].rolling(5).std().bfill()\n",
    "        data['rolling_diff_2_std'] = data['diff_2'].rolling(5).std().bfill()\n",
    "        data['rolling_diff_3_std'] = data['diff_3'].rolling(5).std().bfill()\n",
    "        data['rolling_diff_4_std'] = data['diff_4'].rolling(5).std().bfill()\n",
    "        data['rolling_diff_5_std'] = data['diff_5'].rolling(5).std().bfill()\n",
    "        data['rolling_diff_6_std'] = data['diff_6'].rolling(5).std().bfill()\n",
    "        data['rolling_diff_7_std'] = data['diff_7'].rolling(5).std().bfill()\n",
    "        \n",
    "        data['driver'] = 0 if driver == 'A' else 1\n",
    "        \n",
    "        test.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cbce6c",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab5dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(df, column1, column2):\n",
    "    df[column1 + '-' + column2] = 0\n",
    "    df[column1 + '-' + column2] = df[column1] - df[column2]\n",
    "    return df[column1 + '-' + column2]\n",
    "\n",
    "def sum_2(df, column1, column2):\n",
    "    df[column1 + '+' + column2] = 0\n",
    "    df[column1 + '+' + column2] = df[column1] + df[column2]\n",
    "    return df[column1 + '+' + column2]\n",
    "\n",
    "def sum_3(df, column1, column2, column3):\n",
    "    df[column1 + '+' + column2 + '+' + column3] = 0\n",
    "    df[column1 + '+' + column2 + '+' + column3] = df[column1] + df[column2] + df[column3]\n",
    "    return df[column1 + '+' + column2 + '+' + column3]\n",
    "\n",
    "def sum_4(df, column1, column2, column3, column4):\n",
    "    df[column1 + '+' + column2 + '+' + column3 + '+' + column4] = 0\n",
    "    df[column1 + '+' + column2 + '+' + column3 + '+' + column4] = df[column1] + df[column2] + df[column3] + df[column4]\n",
    "    return df[column1 + '+' + column2 + '+' + column3 + '+' + column4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c0782",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['time_to_minute'] = train['time']//60\n",
    "train['time_to_second'] = train['time']%60\n",
    "\n",
    "train['sigA-sigB'] = diff(train,'signal_A','signal_B')\n",
    "train['sigB-sigC'] = diff(train,'signal_B','signal_C')\n",
    "train['sigC-sigA'] = diff(train,'signal_C','signal_A')\n",
    "\n",
    "train['sigA+sigB'] = sum_2(train,'signal_A','signal_B')\n",
    "train['sigB+sigC'] = sum_2(train,'signal_B','signal_C')\n",
    "train['sigC+sigA'] = sum_2(train,'signal_C','signal_A')\n",
    "\n",
    "train['sigA+sigB+sigC'] = sum_3(train,'signal_A','signal_B','signal_C')\n",
    "\n",
    "train['senA-senB'] = diff(train,'sensor_A','sensor_B')\n",
    "train['senA-senC'] = diff(train,'sensor_A','sensor_C')\n",
    "train['senA-senD'] = diff(train,'sensor_A','sensor_D')\n",
    "train['senB-senC'] = diff(train,'sensor_B','sensor_C')\n",
    "train['senB-senD'] = diff(train,'sensor_B','sensor_D')\n",
    "train['senC-senD'] = diff(train,'sensor_C','sensor_D')\n",
    "\n",
    "train['senA+senB'] = sum_2(train,'sensor_A','sensor_B')\n",
    "train['senA+senC'] = sum_2(train,'sensor_A','sensor_C')\n",
    "train['senA+senD'] = sum_2(train,'sensor_A','sensor_D')\n",
    "train['senB+senC'] = sum_2(train,'sensor_B','sensor_C')\n",
    "train['senB+senD'] = sum_2(train,'sensor_B','sensor_D')\n",
    "train['senC+senD'] = sum_2(train,'sensor_C','sensor_D')\n",
    "\n",
    "train['senA+senB+senC'] = sum_3(train,'sensor_A','sensor_B','sensor_C')\n",
    "train['senA+senB+senD'] = sum_3(train,'sensor_A','sensor_B','sensor_D')\n",
    "train['senB+senC+senD'] = sum_3(train,'sensor_B','sensor_C','sensor_D')\n",
    "\n",
    "train['senA+senB+senC+senD'] = sum_4(train,'sensor_A','sensor_B','sensor_C','sensor_D')\n",
    "\n",
    "train = train.drop(columns = ['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c11a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new = []\n",
    "for data in tqdm(test):\n",
    "    data.columns = ['time','signal_A','signal_B','signal_C','sensor_A','sensor_B','sensor_C','sensor_D',\n",
    "                    'diff_1', 'diff_2', 'diff_3', 'diff_4','diff_5', 'diff_6', 'diff_7', \n",
    "                    'rolling_diff_1', 'rolling_diff_2','rolling_diff_3', 'rolling_diff_4', 'rolling_diff_5', 'rolling_diff_6','rolling_diff_7',\n",
    "                    'rolling_diff_1_std','rolling_diff_2_std','rolling_diff_3_std','rolling_diff_4_std','rolling_diff_5_std','rolling_diff_6_std','rolling_diff_7_std',\n",
    "                    'driver']\n",
    "    data['time_to_minute'] = data['time']//60\n",
    "    data['time_to_second'] = data['time']%60\n",
    "    \n",
    "    data['sigA-sigB'] = diff(data,'signal_A','signal_B')\n",
    "    data['sigB-sigC'] = diff(data,'signal_B','signal_C')\n",
    "    data['sigC-sigA'] = diff(data,'signal_C','signal_A')\n",
    "\n",
    "    data['sigA+sigB'] = sum_2(data,'signal_A','signal_B')\n",
    "    data['sigB+sigC'] = sum_2(data,'signal_B','signal_C')\n",
    "    data['sigC+sigA'] = sum_2(data,'signal_C','signal_A')\n",
    "\n",
    "    data['sigA+sigB+sigC'] = sum_3(data,'signal_A','signal_B','signal_C')\n",
    "\n",
    "    data['senA-senB'] = diff(data,'sensor_A','sensor_B')\n",
    "    data['senA-senC'] = diff(data,'sensor_A','sensor_C')\n",
    "    data['senA-senD'] = diff(data,'sensor_A','sensor_D')\n",
    "    data['senB-senC'] = diff(data,'sensor_B','sensor_C')\n",
    "    data['senB-senD'] = diff(data,'sensor_B','sensor_D')\n",
    "    data['senC-senD'] = diff(data,'sensor_C','sensor_D')\n",
    "\n",
    "    data['senA+senB'] = sum_2(data,'sensor_A','sensor_B')\n",
    "    data['senA+senC'] = sum_2(data,'sensor_A','sensor_C')\n",
    "    data['senA+senD'] = sum_2(data,'sensor_A','sensor_D')\n",
    "    data['senB+senC'] = sum_2(data,'sensor_B','sensor_C')\n",
    "    data['senB+senD'] = sum_2(data,'sensor_B','sensor_D')\n",
    "    data['senC+senD'] = sum_2(data,'sensor_C','sensor_D')\n",
    "\n",
    "    data['senA+senB+senC'] = sum_3(data,'sensor_A','sensor_B','sensor_C')\n",
    "    data['senA+senB+senD'] = sum_3(data,'sensor_A','sensor_B','sensor_D')\n",
    "    data['senB+senC+senD'] = sum_3(data,'sensor_B','sensor_C','sensor_D')\n",
    "\n",
    "    data['senA+senB+senC+senD'] = sum_4(data,'sensor_A','sensor_B','sensor_C','sensor_D')\n",
    "    data = data.drop(columns = ['time'])\n",
    "    test_new.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0248803",
   "metadata": {},
   "source": [
    "## Modeling - LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b159aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns= ['label'])\n",
    "y = train['label'].values\n",
    "X_test = test_new.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c862409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경고 끄기\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "models = []\n",
    "mae_scores = []\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, y, test_size = 0.02, stratify = y, random_state=42)\n",
    "\n",
    "model = LGBMRegressor(boosting_type='gbdt',\n",
    "                    objective='tweedie', \n",
    "                    n_estimators=1500,\n",
    "                    max_depth=10,\n",
    "                    learning_rate=0.3,\n",
    "                    colsample_bytree=0.9,\n",
    "                    subsample=1.0,\n",
    "                    min_child_weight=150,\n",
    "                    num_leaves=16,\n",
    "                    reg_alpha=20,\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42) \n",
    "\n",
    "model.fit(X_train, Y_train, eval_set=[(X_train, Y_train), (X_valid, Y_valid)], early_stopping_rounds=50, verbose=100)\n",
    "\n",
    "pred = model.predict(X_valid)\n",
    "score = mean_absolute_error(Y_valid, pred)\n",
    "print(f\"Tweedie Validation MAE score: {score}\")\n",
    "models.append(model)\n",
    "\n",
    "model = LGBMRegressor(boosting_type='gbdt',\n",
    "                    objective='poisson', \n",
    "                    n_estimators=1500,\n",
    "                    max_depth=10,\n",
    "                    learning_rate=0.3,\n",
    "                    colsample_bytree=0.9,\n",
    "                    subsample=1.0,\n",
    "                    min_child_weight=150,\n",
    "                    num_leaves=16,\n",
    "                    reg_alpha=20,\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42) \n",
    "\n",
    "model.fit(X_train, Y_train, eval_set=[(X_train, Y_train), (X_valid, Y_valid)], early_stopping_rounds=50, verbose=100)\n",
    "\n",
    "pred = model.predict(X_valid)\n",
    "score = mean_absolute_error(Y_valid, pred)\n",
    "print(f\"Poisson Validation MAE score: {score}\")\n",
    "models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db372ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importance\n",
    "predictors = X.columns\n",
    "tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': models[0].feature_importances_})\n",
    "tmp = tmp.sort_values(by='Feature importance',ascending=False)\n",
    "plt.figure(figsize = (7,4))\n",
    "plt.title('Features importance',fontsize=7)\n",
    "s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n",
    "s.set_xticklabels(s.get_xticklabels(),rotation=90, size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deec5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importance\n",
    "predictors = X.columns\n",
    "tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': models[1].feature_importances_})\n",
    "tmp = tmp.sort_values(by='Feature importance',ascending=False)\n",
    "plt.figure(figsize = (7,4))\n",
    "plt.title('Features importance',fontsize=7)\n",
    "s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n",
    "s.set_xticklabels(s.get_xticklabels(),rotation=90, size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc722ee3",
   "metadata": {},
   "source": [
    "## Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f25621",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_final = []\n",
    "for test in tqdm(X_test):\n",
    "    test.columns = X.columns\n",
    "    pred = models[0].predict(test).mean()*0.5 + models[1].predict(test).mean()*0.5\n",
    "    preds_final.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2204b39",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "\n",
    "# 결과 후처리\n",
    "preds_final_new = np.round(preds_final, 0).astype(int)\n",
    "\n",
    "submit['weight'] = preds_final_new\n",
    "submit.loc[submit['weight'] > 400, 'weight'] = submit.loc[submit['weight'] > 400,]['weight'].apply(lambda x : np.round(x, -2))\n",
    "\n",
    "submit.to_csv('./LGBM_58.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87995a59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
